package netflix.recommender;

//Debugging is twice as hard as writing the code in the first place. Therefore, 
//if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.

import java.text.DecimalFormat;
import netflix.memreader.*;
import netflix.algorithms.memorybased.memreader.MyRecommender;
import netflix.memreader.Sparsity;

public class CallRecommenders 

{

   	    String t; 
	    String tr;
	    String p; 
	    String f;
	    String storedt;
	    String storedtr;
	    String what;				
	    String fileNameToWrite;
	    
	    int 	myNeighbourSize;
	    int		myNeighbourInc;
	    String 	myType;				//type of recommender algo
	    double  factor;				//division factor train size
	    int 	topN;				//topN
	    Sparsity mySp;
	    MemHelper mhh;
	    double    ss;

	    boolean mySmallCall;
	    boolean mySparseCall;
	    boolean myTopCall;
	    int     myLoopIndex;
	    
	    
    public CallRecommenders()
    
    {
    				//used to call some functions like calculate sparsity
    	
    }
	    
/*******************************************************************************************************/
    
 /**
 * @param small
 * @param sparseCall
 * @param top
 * @param topNumber
 * @param trainNumber
 * @param loopIndex
 * 
 */	    
	public CallRecommenders(boolean small,			// small or large set
							boolean sparseCall,		// if sparse call, rather than simple data
							boolean top, 			// wanna see top users?
							int topNumber, 		    // how much training set (e.g.80) 
							int trainNumber,		// top N 
							int loopIndex)			//require to calculate the sparse results	
	{
	
		    mySp = new Sparsity();	
		
			myNeighbourSize = 500;
			myNeighbourInc  = 10;
			factor			= (trainNumber * (1.0))/100.0;
			topN			= topNumber;
			myType			= "simple";
			//myType			= "deviation";
			

			 mySmallCall 		= small;
		     mySparseCall 		= sparseCall;
		     myTopCall 			= top;
		     myLoopIndex 		= loopIndex;
	}
	
		    
/*******************************************************************************************************/
	
	    public void callsetUpFunction
	    
	    				(boolean small,			// small or large set
						 String  whichCall,		// Contains sparse etc
	    				 int topNumber, 		// how much training set (e.g.80) 
						 int trainNumber,		// top N 
						 int loopIndex,      	// require to calculate the sparse results
						 int factorX			// to determine which of the test-train ratio is theer
	    				)		

 {
		     

			factor			= (trainNumber * (1.0))/100.0;
			topN			= topNumber;
			myType			= "simple";
			//myType			= "deviation";

			
		if (small ==true)
			
		{
					
						//All users
			//10-90
			//20-80
			//30-70
			
		
			
			//all, non-sparse, cross
		if (whichCall.equalsIgnoreCase("AllNonSparseCross")) //here we will check cross validation
				 
			  {
			
				 p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\Trainer\\";
				 f = p + "sml_storedRatings.dat";
				
				    
			  //  t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
		       // tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
				
				 //we have everything in the folder we just need to call recommender function with suitable names
				storedt	 = p + "sml_testSetStoredFold" + loopIndex + ".dat";			
				storedtr = p + "sml_trainSetStoredFold" + loopIndex + ".dat";
				what 	 ="ALL sml with  10 fold cross validation in 80% of traing set for checking parameter sensitivity";
				fileNameToWrite = p + "\\Results\\" + myType + "Fold" + loopIndex + ".dat";
			 }
			
			
		if (whichCall.equalsIgnoreCase("AllNonSparse")) 
		 
		  {
		
			 p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\";
			 f = p + "sml_storedRatings.dat";
			
			    
		    t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
	        tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
			storedt	 = p + "\\WrittenFiles\\"+"sml_testSetStoredAll" + "_" + (100-trainNumber) + ".dat";			
			storedtr = p + "\\WrittenFiles\\"+"sml_trainSetStoredAll" + "_" + trainNumber + ".dat";
			what 	 ="ALL sml with " + trainNumber + " train and " + (100-trainNumber) +" test data";
			fileNameToWrite = p + "\\Results\\" + myType + "All" + "_" + trainNumber + ".dat";
		 }
		  
		if (whichCall.equalsIgnoreCase("AllSparse"))	//now we will play with sparse data
			
		  {
			    p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\Sparsity\\";
				f = p + "sml_storedRatings.dat";
								    
			    t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
		        tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
				storedt	 = p + "\\WrittenFiles\\"+"sml_testSetStoredaAll" + "_" + (100-trainNumber) + ".dat";
				
				storedtr = p + "\\WrittenFiles\\"+"sml_trainSetStoredAll" + "_" + trainNumber + "_" + (loopIndex*10) + ".dat";
				what 	 ="ALL sml with " + trainNumber + " train and " + (100-trainNumber) +" test data" + " 10% sparsity";
				mhh= new MemHelper(storedtr);
				ss = mySp.calculateSparsity(mhh);
				fileNameToWrite = p + "\\Results\\" +  myType + "All" + "_" + trainNumber + "_" + ss + ".dat";
			  
			  			  
		  }
		 
	   
		
		//users who saw > N movies
		//10-90
		//20-80
		//30-70
		
		if (whichCall.equalsIgnoreCase("TopNonSparse"))
			
			{
			
				p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\";
				f = p + "sml_storedRatings" + topNumber + ".dat";
			
			
				t  		 = p + "sml_testSetTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				tr 		 = p + "sml_trainSetTop" + topNumber +  "_" + trainNumber + ".dat";
				
				storedt	 = p + "\\WrittenFiles\\" + "sml_testSetStoredTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				storedtr = p + "\\WrittenFiles\\"+ "sml_trainSetStoredTop" + topNumber + "_" + trainNumber + ".dat";
				what 	 ="Top"  + topNumber +  "sml with " + trainNumber + " train and " + (100-trainNumber) +" test data";
				fileNameToWrite = p + "\\Results\\"+ myType + topNumber + "_" + trainNumber + ".dat";
			} 
			
		if (whichCall.equalsIgnoreCase("TopSparse")) 	//it is a sparse call with top 20 movies
			
			{
				p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\Sparsity\\";
				f = p + "sml_storedRatings" + topNumber + ".dat";
			
			
				t  		 = p + "sml_testSetTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				tr 		 = p + "sml_trainSetTop" + topNumber +  "_" + trainNumber + ".dat";
				storedt	 = p + "\\WrittenFiles\\"+"sml_testSetStoredTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				
				//this should change in every step
				storedtr = p + "\\WrittenFiles\\" + "sml_trainSetStoredTop" + topNumber + "_" + trainNumber + "_" + (loopIndex *10) + ".dat";
				what 	 ="Top"  + topNumber +  "sml with " + trainNumber + " train and " + (100-trainNumber) +" test data" + (loopIndex*10) + "% Sparsity"; 
				mhh= new MemHelper(storedtr);
				ss = mySp.calculateSparsity(mhh); 
				fileNameToWrite = p + "\\Results\\" + myType + topNumber + "_" + trainNumber + "_" + ss + ".dat";
			
				
			}
		

	}//end of small
		
		
		else //ML 
		
		{
			
			

			//All users
			//10-90
			//20-80
			//30-70
			
	/*	if(top == false) 
		
		 {
			p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\ML_ML\\TestTrain\\";
			f = p + "ml_storedRatings.dat";
			
			    
		    t  		 = p + "ml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";
	        tr 		 = p + "ml_trainSetAll.dat"+ "_" + trainNumber + ".dat";
			storedt	 = p + "ml_testSetStoredaAll" + "_" + (100-trainNumber) + ".dat";
			storedtr = p + "ml_trainSetStoredAll" + "_" + trainNumber + ".dat";
			what 	 ="ALL ml with " + trainNumber + " train and " + (100-trainNumber) +" test data";
			fileNameToWrite = p + "Results" + myType + "All" + "_" + trainNumber + ".dat";
		 }
		
		//users who saw > N movies
		//10-90
		//20-80
		//30-70
		
		else 
		{
			p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\ML_ML\\TestTrain\\";
			f = p + "ml_storedRatings.dat";
			
			t  		 = p + "ml_testSetTop" + topNumber + "_" + (100-trainNumber) + ".dat";
	        tr 		 = p + "ml_trainSetTop" + topNumber + "_" + trainNumber + ".dat";
			storedt	 = p + "ml_testSetStored" + topNumber + "_" + (100-trainNumber) + ".dat";
			storedtr = p + "ml_trainSetStoredTop20" + topNumber + "_" + trainNumber + ".dat";
			what 	 ="Top"  + topNumber +  "ml with " + trainNumber + " train and " + (100-trainNumber) +" test data";
			fileNameToWrite = p + "\\Results\\" + myType + topNumber + "_" + trainNumber + ".dat";
		}
	*/

	}
		
		
			
		    
	}
	
	
/******************************************************************************************/
	
	public static void main (String arg[])
	
	{
	
		boolean smallCall	=true;
		boolean sparseCall	=false;
		boolean topCall  	=false;
		boolean crossCall  	=false;
		
		String myCallChoice ="";
		
		int topNo	 		=20;		
		int train			=80;		//by default for sparsity we are making 20-80%
		double sparseWrite  =0.0;
		
		//I am considering only small set and topset with top20
		
		CallRecommenders rrr = new CallRecommenders();
		
		
		CallRecommenders cr = new CallRecommenders(smallCall,
													sparseCall,
													topCall,
													topNo,
													train,
													0
													);
		
		
	      //_____________________________________________________________________
	     // 
		 //_____________________________________________________________________
		
/*		
 			myCallChoice = "sparse";
			if (sparseCall == false) //we make experiments with neighbourhood size
				
				{ 
				
				   for (int j = 90; j>50; j-=10) // train size = 90,80,70 ...
				
					
					{
						train=j;
						
						cr.callsetUpFunction  ( smallCall,
												sparseCall,
												topCall,
												crossCall,
												topNo,
												train,
												 0
												);
				
						cr.call();
					
					}

				}//end of if
	*/		
			//_____________________________________________________________________
		     // 
			 //_____________________________________________________________________
			
			
			
	/*		else	//now we will simply play with sparse data (THis is called once) 
			  
			 {
				 for (int j = 1; j <10; j++) //there are 9 sparse dataset against each (i.e. all, and top20) 
				 
				 {	 
					    					 
					 cr.callsetUpFunction ( smallCall,
											sparseCall,
											topCall,
											crossCall,
											topNo,
											train,
											j 
										  );

						cr.call();
				 }
				 
			  }//end of sparse call
		*/		
			//_____________________________________________________________________
		     // corss validation with data set 20 test and 80 train 
			 // train is further divided into 10- fold corss validation
			 // to check up for paramter sensitivity
			 //_____________________________________________________________________
	/*		
		
		myCallChoice = "AllNonSparseCross";
		
		for (int i=0; i<9; i++)				// for factor x (test train ration) x=0.1 means 10% test set
		{
		 for (int j = 0; j <10; j++)      //there are 10 folds 
			 
		 {	 
			    					 
			 cr.callsetUpFunction ( smallCall,
									myCallChoice,
					 				topNo,
									train,
									j+1,
									i+1
								  );

				cr.call();
		 }
		 
		}
		
		*/
		
	}//end of main
	
	
/******************************************************************************************/
	
	public void call()
	
	{
		
		//objects of related classes
		DivideIntoSets div 			= new DivideIntoSets();
		SimpleDivideIntoSets Sdiv 	= new SimpleDivideIntoSets();
		MemReader       mr		    = new MemReader ();
		MyRecommender   myRec 		= new MyRecommender();
		AnalyzeAFile    ana			= new AnalyzeAFile();
		
		double sparse =0.0;
			
		
		// divide main file into test and train file
	/*	div.divideIntoTestTrain(f,							// main object already there
								tr, 						// training object to be written	
								t, 							// test object to be written	
								factor); 					// train-test propotion factor
		
	//	Sdiv.divideIntoTestTrain(f, tr, t, 0.8); 			//wtf....good results
		System.out.println("Done division");
	
		
		mr.writeIntoDisk(t, storedt); 							//write test set into memory
		mr.writeIntoDisk(tr, storedtr);							//write train set into memory
		
		
	//	ana.analyzeContent(storedtr);							//analyze the content of training object	
		sparse = ana.calculateSparsity(storedtr);				//calculate sparsity level
		
		System.out.println("Done writing");

		*/
/*
		myRec.makeCorrPrediction(storedtr,		 //training set object 
				storedt, 						 //test set object
				p, 								 //path where these objects are present
				what + ", Sparsity = "+ sparse,  //information about training set
				false, 							 //write Roc related results into the file?
				false,							 //write REComendation related results into the file?
				myType,							 //"simple", "deviation"
				myNeighbourSize,
				myNeighbourInc
				);		

	*/	
		
		myRec.makeCorrPrediction(storedtr,						 //training set object 
								storedt, 						 //test set object
								p, 								 //path where these objects are present
								what + ", Sparsity = "+ sparse,  //information about training set
								false, 							 //write Roc related results into the file?
								true,							 //write REComendation related results into the file?
								myType,							 //"simple", "deviation"
								fileNameToWrite,				 //write results in this file
								myNeighbourSize,
								myNeighbourInc
								);		
			
		
	}
	
/*****************************************************************************************************/
	
	
	
}
