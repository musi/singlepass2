package netflix.recommender;

//Debugging is twice as hard as writing the code in the first place. Therefore, 
//if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.

import java.text.DecimalFormat;
import netflix.FtMemreader.*;
import netflix.algorithms.memorybased.ftmemreader.FTNDepthRec;
import netflix.memreader.Sparsity;

public class CallFTNDepthRec

{

   	    String t; 
	    String tr;
	    String p; 
	    String f;
	    String storedt;
	    String storedtr;
	    String what;				
	    String fileNameToWrite;
	    
	    int 	myNeighbourSize;
	    int		myNeighbourInc;
	    String 	myType;				//type of recommender algo
	    double  factor;				//division factor train size
	    int 	topN;				//topN
	    Sparsity mySp;
	    FTMemHelper mhh;
	    double    ss;

	    boolean mySmallCall;
	    boolean mySparseCall;
	    boolean myTopCall;
	    int     myLoopIndex;
	    
	    
    public CallFTNDepthRec()
    
    {
    				//used to call some functions like calculate sparsity
    	
    }
	    
/*******************************************************************************************************/
    
 /**
 * @param small
 * @param sparseCall
 * @param top
 * @param topNumber
 * @param trainNumber
 * @param loopIndex
 * 
 */	    
	
   public CallFTNDepthRec(boolean small,			// small or large set
							boolean sparseCall,		// if sparse call, rather than simple data
							boolean top, 			// wanna see top users?
							int topNumber, 		    // how much training set (e.g.80) 
							int trainNumber,		// top N 
							int loopIndex)			//require to calculate the sparse results	
	{
	
		    mySp = new Sparsity();	
		
			myNeighbourSize = 150;
			myNeighbourInc  = 20;
			factor			= (trainNumber * (1.0))/100.0;
			topN			= topNumber;
			//myType			= "simple";
		    //myType			= "deviation";
			

			 mySmallCall 		= small;
		     mySparseCall 		= sparseCall;
		     myTopCall 			= top;
		     myLoopIndex 		= loopIndex;
	}
	
		    
/*******************************************************************************************************/
	
	    public void callsetUpFunction	    
	    				(boolean small,			// small or large set
						 String  whichCall,		// Contains sparse etc
	    				 int topNumber, 		// how much training set (e.g.80) 
						 int trainNumber,		// top N 
						 int loopIndex,      	// require to calculate the sparse results
						 int factorX			// to determine which of the test-train ratio is there
	    				)		

 {			factor			= (trainNumber * (1.0))/100.0;
			topN			= topNumber;
			
		//	myType		= "deviation";

			
		if (small ==true)
			
		{
					
						//All users
			//10-90
			//20-80
			//30-70
			
		
			
			//all, non-sparse, cross
		if (whichCall.equalsIgnoreCase("AllNonSparseCrossNeighbour")) //here we will check cross validation
				 
			  {
				String neighbourz = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\Neighbourhood\\";
				// p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\TenFoldData\\";
				 p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\FT\\TestTrain\\FiveFoldData\\";
				f = p + "sml_storedRatings.dat";
				
				    
			  //  t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
		       // tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
				
				 //we have everything in the folder we just need to call recommender function with suitable names
				storedt	 = p + "Data1\\"+ "ft_testSetStoredFold" + loopIndex + ".dat";			
				storedtr = p + "Data1\\"+ "ft_trainSetStoredFold" + loopIndex + ".dat";
				what 	 ="ALL sml with  10 fold cross validation in 80% of traing set for checking parameter sensitivity";
//				fileNameToWrite = neighbourz + "\\Results\\" + myType + "Fold" + loopIndex;
				fileNameToWrite = p + "\\Results1\\" + myType + "Fold" + loopIndex;
			 }
			
		
		//top, non-sparse, cross
		if (whichCall.equalsIgnoreCase("TopNonSparseCrossNeighbour")) //here we will check cross validation
				 
			  {
				String neighbourz = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\Neighbourhood\\";
				// p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\TenFoldData\\";
				 p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\FT\\TestTrain\\FiveFoldData\\";
				f = p + "sml_5StoredRatings.dat";
				
				    
			  //  t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
		       // tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
				
				 //we have everything in the folder we just need to call recommender function with suitable names
				storedt	 = p + "Data2\\"+ "ft_testSetStoredFold" + loopIndex + ".dat";			
				storedtr = p + "Data2\\"+ "ft_trainSetStoredFold" + loopIndex + ".dat";
				what 	 ="ALL sml with  10 fold cross validation in 80% of traing set for checking parameter sensitivity";
//				fileNameToWrite = neighbourz + "\\Results\\" + myType + "Fold" + loopIndex;
				fileNameToWrite = p + "\\Results2\\" + myType + "Fold" + loopIndex;
			 }
			
			
			
		if (whichCall.equalsIgnoreCase("AllNonSparseNeighbour")) 
		 
		  {
		
			 p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\Neighbourhood\\";
			 f = p + "sml_storedRatings.dat";
			
			    
		    t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
	        tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
			storedt	 = p + "\\WrittenFiles\\"+"sml_testSetStoredAll" + "_" + (100-trainNumber) + ".dat";			
			storedtr = p + "\\WrittenFiles\\"+"sml_trainSetStoredAll" + "_" + trainNumber + ".dat";
			what 	 ="ALL sml with " + trainNumber + " train and " + (100-trainNumber) +" test data";
			fileNameToWrite = p + "\\Results\\" + myType + "All" + "_" + trainNumber + ".dat";
		 }
		  
		if (whichCall.equalsIgnoreCase("AllSparse"))	//now we will play with sparse data
			
		  {
			    p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\Sparsity\\";
				f = p + "sml_storedRatings.dat";
								    
	//		    t  		 = p + "sml_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
	//	        tr 		 = p + "sml_trainSetAll.dat" + "_" + trainNumber + ".dat";
				storedt	 = p + "\\WrittenFiles\\"+"sml_testSetStoredaAll" + "_" + (100-trainNumber) + ".dat";
				
				storedtr = p + "\\WrittenFiles\\"+"sml_trainSetStoredAll" + "_" + (trainNumber) + "_" + (loopIndex) + ".dat";
				what 	 ="ALL sml with " + trainNumber + " train and " + (100-trainNumber) +" test data" + " 10% sparsity";
				mhh= new FTMemHelper(storedtr);
			//	ss = mySp.calculateSparsity(mhh);
				fileNameToWrite = p + "\\Results\\" +  myType + "All" + "_" + trainNumber + "_" + ss ;
			  
			  			  
		  }
		 
		//--------------------------------------------------------
	   
		if (whichCall.equalsIgnoreCase("AllSimpleCheck")) //here we will chcek simple rmse, user and movie average
     	  {
	
			p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\FT\\TestTrain\\Depth\\";
			f = p + "ft_storedRatings.dat";
			
			
			    
		    t  		 =  p + "Data1\\" + "ft_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
	        tr 		 =  p + "Data1\\" + "ft_trainSetAll.dat" + "_" + trainNumber + ".dat";
			
			 //we have everything in the folder we just need to call recommender function with suitable names
			storedt	 = f;			
			storedtr = f;
			what 	 ="ALL sml with  10 fold cross validation in 80% of traing set for checking parameter sensitivity";
			fileNameToWrite = p + "\\Results1\\" + myType ;
		 }
	
		
		//--------------------------------------------------------
		
		
		if (whichCall.equalsIgnoreCase("TopSimpleCheck")) //here we will chcek simple rmse, user and movie average
   	  {
	
			p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\FT\\TestTrain\\SimpleCheck\\";
			//f = p + "ft_storedRatings.dat";
			f = p + "ft_only5StoredRatings.dat";
			
			    
		    t  		 =  p + "Data1\\" + "ft_testSetAll.dat"+ "_" + (100-trainNumber) + ".dat";;
	        tr 		 =  p + "Data1\\" + "ft_trainSetAll.dat" + "_" + trainNumber + ".dat";
			
			 //we have everything in the folder we just need to call recommender function with suitable names
			storedt	 =  f;			
			storedtr =  f;
			what 	 ="ALL sml with  10 fold cross validation in 80% of traing set for checking parameter sensitivity";
			fileNameToWrite = p + "\\Results2\\";
		 }
		
		//users who saw > N movies
		//10-90
		//20-80
		//30-70
		
		if (whichCall.equalsIgnoreCase("TopNonSparse"))
			
			{
			
				p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\TestTrain\\";
				f = p + "sml_storedRatings" + topNumber + ".dat";
			
			
				t  		 = p + "sml_testSetTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				tr 		 = p + "sml_trainSetTop" + topNumber +  "_" + trainNumber + ".dat";
				
				storedt	 = p + "\\WrittenFiles\\" + "sml_testSetStoredTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				storedtr = p + "\\WrittenFiles\\"+ "sml_trainSetStoredTop" + topNumber + "_" + trainNumber + ".dat";
				what 	 ="Top"  + topNumber +  "sml with " + trainNumber + " train and " + (100-trainNumber) +" test data";
				fileNameToWrite = p + "\\Results\\"+ myType + topNumber + "_" + trainNumber + ".dat";
			} 
			
		if (whichCall.equalsIgnoreCase("TopSparse")) 	//it is a sparse call with top 20 movies
			
			{
				p = "C:\\Users\\Musi\\workspace\\MusiRecommender\\DataSets\\SML_ML\\Sparsity\\";
				f = p + "sml_storedRatings" + topNumber + ".dat";
			
			
				t  		 = p + "sml_testSetTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				tr 		 = p + "sml_trainSetTop" + topNumber +  "_" + trainNumber + ".dat";
				storedt	 = p + "\\WrittenFiles\\"+"sml_testSetStoredTop" + topNumber + "_" + (100-trainNumber) + ".dat";
				
				//this should change in every step
				storedtr = p + "\\WrittenFiles\\" + "sml_trainSetStoredTop" + topNumber + "_" + trainNumber + "_" + (loopIndex *10) + ".dat";
				what 	 ="Top"  + topNumber +  "sml with " + trainNumber + " train and " + (100-trainNumber) +" test data" + (loopIndex*10) + "% Sparsity"; 
				mhh= new FTMemHelper(storedtr);
		//		ss = mySp.calculateSparsity(mhh); 
				fileNameToWrite = p + "\\Results\\" + myType + topNumber + "_" + trainNumber + "_" + ss + ".dat";
			
				
			}
		

	}//end of small
		
				    
	}
	
	
/******************************************************************************************/
	
	public static void main (String arg[])
	
	{
	
		boolean smallCall	=true;
		boolean sparseCall	=false;
		boolean topCall  	=false;
		boolean crossCall  	=false;
		
		String myCallChoice ="";
		
		int topNo	 		=20;		
		int train			=80;		//by default for sparsity we are making 20-80%
		double sparseWrite  =0.0;
		
		//I am considering only small set and topset with top20
		
		CallFTNDepthRec rrr = new CallFTNDepthRec();
		
		
		CallFTNDepthRec cr = new CallFTNDepthRec(smallCall,
													sparseCall,
													topCall,
													topNo,
													train,
													0
													);

		cr.myType			= "LOO";
		//cr.myType			= "UKFold";
		//cr.myType			= "RKFold";
    	myCallChoice = "AllSimpleCheck";
		smallCall = true;
		train = 80;
		
		{
			
			
		 cr.callsetUpFunction ( smallCall,		// sml_dataset
								myCallChoice,	// which type of data set to call and parameter to check for
				 				topNo,			// top20 or all
								train,			// 80
								0,				// fold index
								0				//used for sparsity cal
							  );

			cr.call();
		}


		
	}//end of main
	
	
/******************************************************************************************/
	
	public void call()	
	{
		
		//objects of related classes
		//DivideIntoSets div 			= new DivideIntoSets();
		//SimpleDivideIntoSets Sdiv 	= new SimpleDivideIntoSets();
		FTMemReader     mr		    = new FTMemReader ();
		FTNDepthRec     myRec 		= new FTNDepthRec();
	//	AnalyzeAFile    ana			= new AnalyzeAFile();
		
		double sparse =0.0;
			
	/*
		// divide main file into test and train file
		div.divideIntoTestTrain(f,							// main object already there
								tr, 						// training object to be written	
								t, 							// test object to be written	
								factor); 					// train-test propotion factor
		
	//	Sdiv.divideIntoTestTrain(f, tr, t, 0.8); 			//wtf....good results
		System.out.println("Done division");
	
		
		mr.writeIntoDisk(t, storedt); 							//write test set into memory
		mr.writeIntoDisk(tr, storedtr);							//write train set into memory
		
		
	//	ana.analyzeContent(storedtr);							//analyze the content of training object	
		sparse = ana.calculateSparsity(storedtr);				//calculate sparsity level
		
		System.out.println("Done writing");

*/
		
		
/*
		myRec.makeCorrPrediction(storedtr,		 //training set object 
				storedt, 						 //test set object
				p, 								 //path where these objects are present
				what + ", Sparsity = "+ sparse,  //information about training set
				false, 							 //write Roc related results into the file?
				false,							 //write REComendation related results into the file?
				myType,							 //"simple", "deviation"
				myNeighbourSize,
				myNeighbourInc
				);		

	*/	
		
	myRec.makeCorrPrediction  (storedtr,						 //training set object 
							   storedt, 						 //test set object
								p, 								 //path where these objects are present
								what + ", Sparsity = "+ sparse,  //information about training set
								true, 							 //write Roc related results into the file?
								true,							 //write REComendation related results into the file?
								myType,							 //"simple", "deviation"
								fileNameToWrite,				 //write results in this file
								myNeighbourSize,
								myNeighbourInc
								);		
			
		
	}
	
/*****************************************************************************************************/
	
	
	
}
